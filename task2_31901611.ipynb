{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Prashasti Garg\n",
    "#### Student ID: 31901611\n",
    "\n",
    "Date: 23/01/2021\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.9 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* pandas (for reading the excel file and sheets) \n",
    "* langid (for classifying the text language)\n",
    "* re (for regular expression) \n",
    "* nltk (for exploring features of raw data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries  \n",
    "- In this task, an excel file is given which consists of 26 sheets. These sheets include the id, date and text of tweets related to covid-19.\n",
    "- In order to extract the data from excel file, pandas is used.\n",
    "- The regex library is used to search a pttern in the texts of the sheets in excel file.\n",
    "- the texts are then checked for english language using langid library.\n",
    "- NLTK is used to work with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import langid\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import *\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Excel file\n",
    "- The data is extracted from excel file using pd.ExcelFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.ExcelFile(r'D:/Jupyter Notebook/Wrangling/Dataset/tweet_dataset.xlsx', engine='openpyxl',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names of the sheets in excel file is stored\n",
    "- The provided sheets in excel file are segregated according to the date of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the sheets are extracted in a variable\n",
    "sheet_names = excel_data.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text from all the sheets is stored in a dictionary\n",
    "- The excel file is parsed, with NaN columns removed using dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# an empty dictionary is created, where dates/sheet name is the key and the text in these respective sheets is the value\n",
    "text_dict = {}\n",
    "for i in sheet_names:\n",
    "    sheet = excel_data.parse(i)\n",
    "    sheet = sheet.dropna(how='all', axis=1)\n",
    "    # removes all the columns with NaN\n",
    "    sheet = sheet[sheet.columns[2]].dropna()\n",
    "    t_list = sheet.values.tolist()\n",
    "    text_dict[i] = t_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text is checked for english language via langid library\n",
    "- The text collected from all the sheets are then checked for english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new dictionary is created to collect all the texts which are in english language\n",
    "en_dict = {}\n",
    "for date, text in text_dict.items():\n",
    "    file = []\n",
    "    for tweet in text:\n",
    "        if langid.classify(tweet)[0] == 'en':\n",
    "            file.append(tweet)   \n",
    "    en_dict[date] = file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens are created from the text in sheets of excel file\n",
    "- Python breaks each logical line into a sequence of elementary lexical components known as tokens. Each token corresponds to a substring of the logical line. The normal token types are identifiers, keywords, operators, delimiters, and literals, as covered in the following sections. (https://www.oreilly.com/library/view/python-in-a/0596100469/ch04s01.html#:~:text=Python%20breaks%20each%20logical%20line,covered%20in%20the%20following%20sections.)\n",
    "- A regex, r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\" is used to search for the tokens with this pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary is formed to collect all the tokens in the text.\n",
    "tokens_dict = {}\n",
    "# en_dict is iterated to create the tokens which are only in english language \n",
    "for date, text in en_dict.items():\n",
    "    tokens_list=[]\n",
    "    for j in text:\n",
    "        tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "        tokens = tokenizer.tokenize(j)\n",
    "        tokens_list += tokens\n",
    "    tokens_dict[date] = tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens are Normalised ie, converted to lower case\n",
    "- All the collected tokens are then converted into lower case to enable the collecting of words easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens_dict = {}\n",
    "for date, text in tokens_dict.items():\n",
    "    lower_tokens_dict[date] = [tokens.lower() for tokens in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created list for stop words \n",
    "- Stop words are the words which occur very frequently in the text, which will create not much difference in the meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an empty list is created to store all the stop words which are extracted from the text file provided\n",
    "stop_words = []\n",
    "# the provided text file of stop words is opened\n",
    "text_file = open(r\"D:/Jupyter Notebook/Wrangling/Dataset/stopwords_en.txt\",encoding=\"utf8\")\n",
    "# each word is iterated in the text_file\n",
    "for word in text_file:\n",
    "    words = word.strip('\\n')\n",
    "    stop_words.append(words)\n",
    "#xtext file is closed\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Independent stop words removed\n",
    "- All the stop words which are not bounded, are removed from the lower_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an independent tokens dictonary is created\n",
    "ind_tokens_dict = {}\n",
    "# date, text are iterared in lower_tokens_dict, where stop are not yet removed\n",
    "for date, text in lower_tokens_dict.items():\n",
    "    ind_tokens = []\n",
    "    for i in text:\n",
    "        if i not in stop_words:\n",
    "            ind_tokens.append(i)\n",
    "    ind_tokens_dict[date] = ind_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Dependent stop words removed\n",
    "- All the tokens with the threshold more than 24 days and the rare tokens with the threshold less than 2 days also the words whose length is less than 3 are all removed from the independent tokens list ie. ind_tokens_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set is created to collect the unique tokens from each date\n",
    "set_text = []\n",
    "for text in ind_tokens_dict.values(): \n",
    "    set_text += list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of set, whose frequency distribution is done, then converted into a dictionary\n",
    "temp = dict(FreqDist(set_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list is created to append all the conditions of dependent tokens\n",
    "vals = []\n",
    "for k, v in temp.items():\n",
    "    if v < 2 or v > 24 or len(k) < 3:\n",
    "        vals.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list is created to gather all the dependent tokens\n",
    "collected_tokens = []\n",
    "for text in ind_tokens_dict.values():\n",
    "    # the tokens which fulfill the conditions are then removed from collected_tokens\n",
    "    if text not in vals:\n",
    "        collected_tokens.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 200 Meaningful Bigram using PMI\n",
    "- Pointwise Mutual Information is used to collect all the 200 bigram tokens.\n",
    "- Collocations are expressions of multiple words which commonly co-occur.(https://www.nltk.org/howto/collocations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for text in lower_tokens_dict.values():\n",
    "    tokens = tokens + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "bigram_200 = bigram_finder.nbest(bigram_measures.pmi, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_200_list = []\n",
    "for _0, _1 in bigram_200:\n",
    "    res = _0 + \"_\" + _1\n",
    "    bigram_200_list.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_flat = [j for i in bigram_200 for j in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemming of tokens\n",
    "- Porter Stemming is done where the stems are removed from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer_list = []\n",
    "for i in collected_tokens:\n",
    "    stem = ['{0}'.format(w, stemmer.stem(w)) for w in i]\n",
    "    stemmer_list.append(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created a stemmed list \n",
    "- The tokens are collected which have been stemmed as well as bigrams which are created using PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [j for i in stemmer_list for j in i if j not in bi_flat] + bigram_200_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vocab_set is sorted\n",
    "vocab_set = list(set(vocab_list))\n",
    "vocab_set.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of each token which are sorted is found using enumerate\n",
    "token_index = list(enumerate(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a final string is created where all the tokens are concatenated\n",
    "final_vocab = \"\"\n",
    "for i, w in token_index:\n",
    "    final_vocab += \"{}:{}\\n\".format(w, i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function is created to create the file for each tak in required format\n",
    "def save_file(file_name, data):\n",
    "    fout = open(file_name, 'w')\n",
    "    fout.write(data)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a .txt file is created to store all the data from final_vocab\n",
    "save_file(\"./31901611_vocab.txt\", final_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 Unigrams\n",
    "- N-grams of texts are extensively used in text mining and natural language processing tasks. An n-gram is a contiguous sequence of n items from a given sample of text or speech. an n-gram of size 1 is referred to as a \"unigram\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = {}\n",
    "for i in range(len(stemmer_list)):\n",
    "    uni_freq = list(dict(FreqDist(stemmer_list[i])).items())\n",
    "    uni_freq.sort(key = lambda x: x[1], reverse = True)\n",
    "    unigram[sheet_names[i]] =  uni_freq[:100]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a .txt file is created to store all the data from unigram dictionary which is converted to string\n",
    "save_file(\"./31901611_100uni.txt\", str(unigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 Bigrams\n",
    "- N-grams of texts are extensively used in text mining and natural language processing tasks. An n-gram is a contiguous sequence of n items from a given sample of text or speech. an n-gram of size 2 is a \"bigram\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = {}\n",
    "for date, text in lower_tokens_dict.items():\n",
    "    bi = ngrams(text, 2)\n",
    "    bi_freq = list(dict(FreqDist(bi)).items())\n",
    "    bi_freq.sort(key = lambda x: x[1], reverse = True)\n",
    "    bigram[date] =  bi_freq[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a .txt file is created to store all the data from bigram dictionary which is later converted to string\n",
    "save_file(\"./31901611_100bi.txt\", str(bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Count Vectors\n",
    "- Create a vector that has as many dimensions as your corpora has unique words. Each unique word has a unique dimension and will be represented by a 1 in that dimension with 0s everywhere else.(https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "joined_data = [' '.join(text) for text in stemmer_list]\n",
    "vectorizer.fit(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "for i in range(len(stemmer_list)):\n",
    "    fd = dict(FreqDist(stemmer_list[i]))\n",
    "    freq_dict[sheet_names[i]] = fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\"\n",
    "for date, text in freq_dict.items():\n",
    "    output += \"{},\".format(date)\n",
    "    for k, v in text.items():\n",
    "        if k in vectorizer.vocabulary_.keys():\n",
    "            ix = vectorizer.vocabulary_[k]\n",
    "            output += \"{}:{},\".format(ix, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a .txt file is created to store all the data from output which collectes the vectors\n",
    "save_file(\"./31901611_countVec.txt\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
